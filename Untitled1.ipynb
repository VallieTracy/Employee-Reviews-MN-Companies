{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from argparse import ArgumentParser\n",
    "import argparse\n",
    "import logging\n",
    "import logging.config\n",
    "from selenium import webdriver as wd\n",
    "import selenium\n",
    "import numpy as np\n",
    "from schema import SCHEMA\n",
    "import json\n",
    "import urllib\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "DEFAULT_URL = ('https://www.glassdoor.com/Overview/Working-at-'\n",
    "               'Premise-Data-Corporation-EI_IE952471.11,35.htm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('-u', '--url',\n",
    "                    help='URL of the company\\'s Glassdoor landing page.',\n",
    "                    default=DEFAULT_URL)\n",
    "parser.add_argument('-f', '--file', default='glassdoor_ratings.csv',\n",
    "                    help='Output file.')\n",
    "parser.add_argument('--headless', action='store_true',\n",
    "                    help='Run Chrome in headless mode.')\n",
    "parser.add_argument('--username', help='Email address used to sign in to GD.')\n",
    "parser.add_argument('-p', '--password', help='Password to sign in to GD.')\n",
    "parser.add_argument('-c', '--credentials', help='Credentials file')\n",
    "parser.add_argument('-l', '--limit', default=25,\n",
    "                    action='store', type=int, help='Max reviews to scrape')\n",
    "parser.add_argument('--start_from_url', action='store_true',\n",
    "                    help='Start scraping from the passed URL.')\n",
    "parser.add_argument(\n",
    "    '--max_date', help='Latest review date to scrape.\\\n",
    "    Only use this option with --start_from_url.\\\n",
    "    You also must have sorted Glassdoor reviews ASCENDING by date.',\n",
    "    type=lambda s: dt.datetime.strptime(s, \"%Y-%m-%d\"))\n",
    "parser.add_argument(\n",
    "    '--min_date', help='Earliest review date to scrape.\\\n",
    "    Only use this option with --start_from_url.\\\n",
    "    You also must have sorted Glassdoor reviews DESCENDING by date.',\n",
    "    type=lambda s: dt.datetime.strptime(s, \"%Y-%m-%d\"))\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.start_from_url and (args.max_date or args.min_date):\n",
    "    raise Exception(\n",
    "        'Invalid argument combination:\\\n",
    "        No starting url passed, but max/min date specified.'\n",
    "    )\n",
    "elif args.max_date and args.min_date:\n",
    "    raise Exception(\n",
    "        'Invalid argument combination:\\\n",
    "        Both min_date and max_date specified.'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.credentials:\n",
    "    with open(args.credentials) as f:\n",
    "        d = json.loads(f.read())\n",
    "        args.username = d['username']\n",
    "        args.password = d['password']\n",
    "else:\n",
    "    try:\n",
    "        with open('secret.json') as f:\n",
    "            d = json.loads(f.read())\n",
    "            args.username = d['username']\n",
    "            args.password = d['password']\n",
    "    except FileNotFoundError:\n",
    "        msg = 'Please provide Glassdoor credentials.\\\n",
    "        Credentials can be provided as a secret.json file in the working\\\n",
    "        directory, or passed at the command line using the --username and\\\n",
    "        --password flags.'\n",
    "        raise Exception(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "logger.addHandler(ch)\n",
    "formatter = logging.Formatter(\n",
    "    '%(asctime)s %(levelname)s %(lineno)d\\\n",
    "    :%(filename)s(%(process)d) - %(message)s')\n",
    "ch.setFormatter(formatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('selenium').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('selenium').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-11 10:54:43,794 INFO 297    :<ipython-input-8-5a7e25d76e7e>(18416) - Configuring browser\n",
      "2020-07-11 10:54:47,070 INFO 337    :<ipython-input-8-5a7e25d76e7e>(18416) - Scraping up to 25 reviews.\n",
      "2020-07-11 10:54:47,078 INFO 278    :<ipython-input-8-5a7e25d76e7e>(18416) - Signing in to vallieetracy@gmail.com\n",
      "2020-07-11 10:54:57,734 INFO 259    :<ipython-input-8-5a7e25d76e7e>(18416) - Navigating to company reviews\n",
      "2020-07-11 10:55:06,449 INFO 208    :<ipython-input-8-5a7e25d76e7e>(18416) - Extracting reviews from page 1\n",
      "2020-07-11 10:55:06,931 INFO 213    :<ipython-input-8-5a7e25d76e7e>(18416) - Found 10 reviews on page 1\n"
     ]
    },
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".common__EiReviewTextStyles__allowLineBreaks\"}\n  (Session info: chrome=83.0.4103.116)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5a7e25d76e7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-5a7e25d76e7e>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    357\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m     \u001b[0mreviews_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_from_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-5a7e25d76e7e>\u001b[0m in \u001b[0;36mextract_from_page\u001b[1;34m()\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_featured\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_review\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m             logger.info(f'Scraped data for \"{data[\"review_title\"]}\"\\\n\u001b[0;32m    219\u001b[0m ({data[\"date\"]})')\n",
      "\u001b[1;32m<ipython-input-8-5a7e25d76e7e>\u001b[0m in \u001b[0;36mextract_review\u001b[1;34m(review)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;31m# import pdb;pdb.set_trace()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfield\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSCHEMA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mauthor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSCHEMA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-5a7e25d76e7e>\u001b[0m in \u001b[0;36mscrape\u001b[1;34m(field, review, author)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0mfdict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSCHEMA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mextract_from_page\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-5a7e25d76e7e>\u001b[0m in \u001b[0;36mscrape_years\u001b[1;34m(review)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscrape_years\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'common__EiReviewTextStyles__allowLineBreaks'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'preceding-sibling::p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mfind_element_by_class_name\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'foo'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \"\"\"\n\u001b[1;32m--> 398\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLASS_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements_by_class_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m         return self._execute(Command.FIND_CHILD_ELEMENT,\n\u001b[1;32m--> 659\u001b[1;33m                              {\"using\": by, \"value\": value})['value']\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".common__EiReviewTextStyles__allowLineBreaks\"}\n  (Session info: chrome=83.0.4103.116)\n"
     ]
    }
   ],
   "source": [
    "def scrape(field, review, author):\n",
    "\n",
    "    def scrape_date(review):\n",
    "        return review.find_element_by_tag_name(\n",
    "            'time').get_attribute('datetime')\n",
    "\n",
    "    def scrape_emp_title(review):\n",
    "        if 'Anonymous Employee' not in review.text:\n",
    "            try:\n",
    "                res = author.find_element_by_class_name(\n",
    "                    'authorJobTitle').text.split('-')[1]\n",
    "            except Exception:\n",
    "                logger.warning('Failed to scrape employee_title')\n",
    "                res = np.nan\n",
    "        else:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_location(review):\n",
    "        if 'in' in review.text:\n",
    "            try:\n",
    "                res = author.find_element_by_class_name(\n",
    "                    'authorLocation').text\n",
    "            except Exception:\n",
    "                res = np.nan\n",
    "        else:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_status(review):\n",
    "        try:\n",
    "            res = author.text.split('-')[0]\n",
    "        except Exception:\n",
    "            logger.warning('Failed to scrape employee_status')\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_rev_title(review):\n",
    "        return review.find_element_by_class_name('summary').text.strip('\"')\n",
    "\n",
    "    def scrape_years(review):\n",
    "        res = review.find_element_by_class_name('common__EiReviewTextStyles__allowLineBreaks').find_element_by_xpath('preceding-sibling::p').text\n",
    "        return res\n",
    "\n",
    "    def scrape_helpful(review):\n",
    "        try:\n",
    "            helpful = review.find_element_by_class_name('helpfulCount')\n",
    "            res = helpful[helpful.find('(') + 1: -1]\n",
    "        except Exception:\n",
    "            res = 0\n",
    "        return res\n",
    "\n",
    "    def expand_show_more(section):\n",
    "        try:\n",
    "            more_content = section.find_element_by_class_name('moreContent') #######################\n",
    "            more_link = section.find_element_by_class_name('v2__EIReviewDetailsV2__continueReading')\n",
    "            more_link.click()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def scrape_pros(review):\n",
    "        try:\n",
    "            pros = review.find_element_by_class_name('common__EiReviewTextStyles__allowLineBreaks')\n",
    "            expand_show_more(pros)\n",
    "            res = pros.text.replace('Pros', '')\n",
    "            res = res.strip()\n",
    "        except Exception:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_cons(review):\n",
    "        try:\n",
    "            cons = review.find_elements_by_class_name('common__EiReviewTextStyles__allowLineBreaks')[1]\n",
    "            expand_show_more(cons)\n",
    "            res = cons.text.replace('Cons', '')\n",
    "            res = res.strip()\n",
    "        except Exception:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_advice(review):\n",
    "        try:\n",
    "            advice = review.find_elements_by_class_name('common__EiReviewTextStyles__allowLineBreaks')[2]\n",
    "            res = advice.text.replace('Advice to Management', '')\n",
    "            res = res.strip()\n",
    "        except Exception:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_overall_rating(review):\n",
    "        try:\n",
    "            ratings = review.find_element_by_class_name('gdStars')\n",
    "            overall = ratings.find_element_by_class_name(\n",
    "                'rating').find_element_by_class_name('value-title')\n",
    "            res = overall.get_attribute('title')\n",
    "        except Exception:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def _scrape_subrating(i):\n",
    "        try:\n",
    "            ratings = review.find_element_by_class_name('gdStars')\n",
    "            subratings = ratings.find_element_by_class_name(\n",
    "                'subRatings').find_element_by_tag_name('ul')\n",
    "            this_one = subratings.find_elements_by_tag_name('li')[i]\n",
    "            res = this_one.find_element_by_class_name(\n",
    "                'gdBars').get_attribute('title')\n",
    "        except Exception:\n",
    "            res = np.nan\n",
    "        return res\n",
    "\n",
    "    def scrape_work_life_balance(review):\n",
    "        return _scrape_subrating(0)\n",
    "\n",
    "    def scrape_culture_and_values(review):\n",
    "        return _scrape_subrating(1)\n",
    "\n",
    "    def scrape_career_opportunities(review):\n",
    "        return _scrape_subrating(2)\n",
    "\n",
    "    def scrape_comp_and_benefits(review):\n",
    "        return _scrape_subrating(3)\n",
    "\n",
    "    def scrape_senior_management(review):\n",
    "        return _scrape_subrating(4)\n",
    "\n",
    "\n",
    "    def scrape_recommends(review):\n",
    "        try:\n",
    "            res = review.find_element_by_class_name('recommends').text\n",
    "            res = res.split('\\n')\n",
    "            return res[0]\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    def scrape_outlook(review):\n",
    "        try:\n",
    "            res = review.find_element_by_class_name('recommends').text\n",
    "            res = res.split('\\n')\n",
    "            if len(res) == 2 or len(res) == 3:\n",
    "                if 'CEO' in res[1]:\n",
    "                    return np.nan\n",
    "                return res[1]\n",
    "            return np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    def scrape_approve_ceo(review):\n",
    "        try:\n",
    "            res = review.find_element_by_class_name('recommends').text\n",
    "            res = res.split('\\n')\n",
    "            if len(res) == 3:\n",
    "                return res[2]\n",
    "            if len(res) == 2:\n",
    "                if 'CEO' in res[1]:\n",
    "                    return res[1]\n",
    "            return np.nan\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "    funcs = [\n",
    "        scrape_date,\n",
    "        scrape_emp_title,\n",
    "        scrape_location,\n",
    "        scrape_status,\n",
    "        scrape_rev_title,\n",
    "        scrape_years,\n",
    "        scrape_helpful,\n",
    "        scrape_pros,\n",
    "        scrape_cons,\n",
    "        scrape_advice,\n",
    "        scrape_overall_rating,\n",
    "        scrape_work_life_balance,\n",
    "        scrape_culture_and_values,\n",
    "        scrape_career_opportunities,\n",
    "        scrape_comp_and_benefits,\n",
    "        scrape_senior_management,\n",
    "        scrape_recommends,\n",
    "        scrape_outlook,\n",
    "        scrape_approve_ceo\n",
    "\n",
    "    ]\n",
    "\n",
    "    fdict = dict((s, f) for (s, f) in zip(SCHEMA, funcs))\n",
    "\n",
    "    return fdict[field](review)\n",
    "\n",
    "def extract_from_page():\n",
    "\n",
    "    def is_featured(review):\n",
    "        try:\n",
    "            review.find_element_by_class_name('featuredFlag')\n",
    "            return True\n",
    "        except selenium.common.exceptions.NoSuchElementException:\n",
    "            return False\n",
    "\n",
    "    def extract_review(review):\n",
    "        author = review.find_element_by_class_name('authorInfo')\n",
    "        res = {}\n",
    "        # import pdb;pdb.set_trace()\n",
    "        for field in SCHEMA:\n",
    "            res[field] = scrape(field, review, author)\n",
    "\n",
    "        assert set(res.keys()) == set(SCHEMA)\n",
    "        return res\n",
    "\n",
    "    logger.info(f'Extracting reviews from page {page[0]}')\n",
    "\n",
    "    res = pd.DataFrame([], columns=SCHEMA)\n",
    "\n",
    "    reviews = browser.find_elements_by_class_name('empReview')\n",
    "    logger.info(f'Found {len(reviews)} reviews on page {page[0]}')\n",
    "\n",
    "    for review in reviews:\n",
    "        if not is_featured(review):\n",
    "            data = extract_review(review)\n",
    "            logger.info(f'Scraped data for \"{data[\"review_title\"]}\"\\\n",
    "({data[\"date\"]})')\n",
    "            res.loc[idx[0]] = data\n",
    "        else:\n",
    "            logger.info('Discarding a featured review')\n",
    "        idx[0] = idx[0] + 1\n",
    "\n",
    "    if args.max_date and \\\n",
    "        (pd.to_datetime(res['date']).max() > args.max_date) or \\\n",
    "            args.min_date and \\\n",
    "            (pd.to_datetime(res['date']).min() < args.min_date):\n",
    "        logger.info('Date limit reached, ending process')\n",
    "        date_limit_reached[0] = True\n",
    "\n",
    "    return res\n",
    "\n",
    "def more_pages():\n",
    "    try:\n",
    "        # paging_control = browser.find_element_by_class_name('pagingControls') \n",
    "        next_ = browser.find_element_by_class_name('pagination__PaginationStyle__next')\n",
    "        next_.find_element_by_tag_name('a')\n",
    "        return True\n",
    "    except selenium.common.exceptions.NoSuchElementException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def go_to_next_page():\n",
    "    logger.info(f'Going to page {page[0] + 1}')\n",
    "    # paging_control = browser.find_element_by_class_name('pagingControls') \n",
    "    next_ = browser.find_element_by_class_name(\n",
    "        'pagination__PaginationStyle__next').find_element_by_tag_name('a')\n",
    "    browser.get(next_.get_attribute('href'))\n",
    "    time.sleep(1)\n",
    "    page[0] = page[0] + 1\n",
    "\n",
    "\n",
    "def no_reviews():\n",
    "    return False\n",
    "    # TODO: Find a company with no reviews to test on\n",
    "\n",
    "def navigate_to_reviews():\n",
    "    logger.info('Navigating to company reviews')\n",
    "\n",
    "    browser.get(args.url)\n",
    "    time.sleep(1)\n",
    "\n",
    "    if no_reviews():\n",
    "        logger.info('No reviews to scrape. Bailing!')\n",
    "        return False\n",
    "\n",
    "    reviews_cell = browser.find_element_by_xpath(\n",
    "        '//a[@data-label=\"Reviews\"]')\n",
    "    reviews_path = reviews_cell.get_attribute('href')\n",
    "\n",
    "    # reviews_path = driver.current_url.replace('Overview','Reviews')\n",
    "    browser.get(reviews_path)\n",
    "    time.sleep(1)\n",
    "    return True\n",
    "                        \n",
    "def sign_in():\n",
    "    logger.info(f'Signing in to {args.username}')\n",
    "\n",
    "    url = 'https://www.glassdoor.com/profile/login_input.htm'\n",
    "    browser.get(url)\n",
    "\n",
    "    # import pdb;pdb.set_trace()\n",
    "\n",
    "    email_field = browser.find_element_by_name('username')\n",
    "    password_field = browser.find_element_by_name('password')\n",
    "    submit_btn = browser.find_element_by_xpath('//button[@type=\"submit\"]')\n",
    "\n",
    "    email_field.send_keys(args.username)\n",
    "    password_field.send_keys(args.password)\n",
    "    submit_btn.click()\n",
    "\n",
    "    time.sleep(3)\n",
    "    browser.get(args.url)\n",
    "                        \n",
    "def get_browser():\n",
    "    logger.info('Configuring browser')\n",
    "    chrome_options = wd.ChromeOptions()\n",
    "    if args.headless:\n",
    "        chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('log-level=3')\n",
    "    browser = wd.Chrome(options=chrome_options)\n",
    "    return browser\n",
    "\n",
    "\n",
    "def get_current_page():\n",
    "    logger.info('Getting current page number')\n",
    "    paging_control = browser.find_element_by_class_name('pagingControls')\n",
    "    current = int(paging_control.find_element_by_xpath(\n",
    "        '//ul//li[contains\\\n",
    "        (concat(\\' \\',normalize-space(@class),\\' \\'),\\' current \\')]\\\n",
    "        //span[contains(concat(\\' \\',\\\n",
    "        normalize-space(@class),\\' \\'),\\' disabled \\')]')\n",
    "        .text.replace(',', ''))\n",
    "    return current\n",
    "                        \n",
    "def verify_date_sorting():\n",
    "    logger.info('Date limit specified, verifying date sorting')\n",
    "    ascending = urllib.parse.parse_qs(\n",
    "        args.url)['sort.ascending'] == ['true']\n",
    "\n",
    "    if args.min_date and ascending:\n",
    "        raise Exception(\n",
    "            'min_date required reviews to be sorted DESCENDING by date.')\n",
    "    elif args.max_date and not ascending:\n",
    "        raise Exception(\n",
    "            'max_date requires reviews to be sorted ASCENDING by date.')\n",
    "\n",
    "\n",
    "browser = get_browser()\n",
    "page = [1]\n",
    "idx = [0]\n",
    "date_limit_reached = [False]\n",
    "                        \n",
    "def main():\n",
    "\n",
    "    logger.info(f'Scraping up to {args.limit} reviews.')\n",
    "\n",
    "    res = pd.DataFrame([], columns=SCHEMA)\n",
    "\n",
    "    sign_in()\n",
    "\n",
    "    if not args.start_from_url:\n",
    "        reviews_exist = navigate_to_reviews()\n",
    "        if not reviews_exist:\n",
    "            return\n",
    "    elif args.max_date or args.min_date:\n",
    "        verify_date_sorting()\n",
    "        browser.get(args.url)\n",
    "        page[0] = get_current_page()\n",
    "        logger.info(f'Starting from page {page[0]:,}.')\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        browser.get(args.url)\n",
    "        page[0] = get_current_page()\n",
    "        logger.info(f'Starting from page {page[0]:,}.')\n",
    "        time.sleep(1)\n",
    "\n",
    "    reviews_df = extract_from_page()\n",
    "    res = res.append(reviews_df)\n",
    "\n",
    "    # import pdb;pdb.set_trace()\n",
    "\n",
    "    while more_pages() and\\\n",
    "            len(res) < args.limit and\\\n",
    "            not date_limit_reached[0]:\n",
    "        go_to_next_page()\n",
    "        reviews_df = extract_from_page()\n",
    "        res = res.append(reviews_df)\n",
    "\n",
    "    logger.info(f'Writing {len(res)} reviews to file {args.file}')\n",
    "    res.to_csv(args.file, index=False, encoding='utf-8')\n",
    "\n",
    "    end = time.time()\n",
    "    logger.info(f'Finished in {end - start} seconds')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
